{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VacuaGym Complete Pipeline - MEMORY-SAFE VERSION\n",
    "\n",
    "**Date**: 2025-12-27  \n",
    "**Status**: OOM-proof - uses streaming for full dataset runs\n",
    "\n",
    "This notebook orchestrates the complete VacuaGym pipeline WITHOUT loading full datasets into RAM:\n",
    "\n",
    "1. **Validation of Current State** (V1)\n",
    "2. **Phase 3 V2**: Calls script (streaming checkpoints)\n",
    "3. **Mid-run Validation**: Streaming stats (no full load)\n",
    "4. **Data Splitting**: Works on indices\n",
    "5. **Baseline Training**: Chunked loading\n",
    "6. **Final Validation**: Streaming stats\n",
    "\n",
    "**Safe for N_LIMIT=None (270k+ samples)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Setup complete\n",
      "  Working directory: /home/tlabib/Documents/github/VacuaGym\n",
      "  Python version: 3.12.3\n",
      "  PyArrow available: True\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import json\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import pyarrow for streaming\n",
    "try:\n",
    "    import pyarrow.parquet as pq\n",
    "    HAVE_PYARROW = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è pyarrow not found - install with: pip install pyarrow\")\n",
    "    HAVE_PYARROW = False\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Paths\n",
    "INPUT_DIR = Path(\"data/processed/tables\")\n",
    "OUTPUT_DIR = Path(\"data/processed/labels\")\n",
    "CHECKPOINT_DIR_V1 = Path(\"data/processed/labels/checkpoints\")\n",
    "CHECKPOINT_DIR_V2 = Path(\"data/processed/labels/checkpoints_v2\")\n",
    "SPLITS_DIR = Path(\"data/processed/splits\")\n",
    "VALIDATION_DIR = Path(\"data/processed/validation\")\n",
    "\n",
    "# Create directories\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CHECKPOINT_DIR_V2.mkdir(parents=True, exist_ok=True)\n",
    "SPLITS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "VALIDATION_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Configuration\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "print(\"‚úì Setup complete\")\n",
    "print(f\"  Working directory: {Path.cwd()}\")\n",
    "print(f\"  Python version: {sys.version.split()[0]}\")\n",
    "print(f\"  PyArrow available: {HAVE_PYARROW}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming Utilities (No OOM)\n",
    "\n",
    "These functions process parquet files in batches without loading everything into RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Streaming utilities defined\n"
     ]
    }
   ],
   "source": [
    "def stream_label_stats(parquet_path, batch_size=50_000):\n",
    "    \"\"\"\n",
    "    Compute statistics from parquet file WITHOUT loading into RAM.\n",
    "    \n",
    "    Args:\n",
    "        parquet_path: Path to parquet file\n",
    "        batch_size: Process this many rows at a time\n",
    "    \n",
    "    Returns:\n",
    "        Dict with stats: total, success_rate, stability_counts, eigenvalue signs\n",
    "    \"\"\"\n",
    "    if not HAVE_PYARROW:\n",
    "        print(\"ERROR: pyarrow required for streaming. Install with: pip install pyarrow\")\n",
    "        return None\n",
    "    \n",
    "    pf = pq.ParquetFile(str(parquet_path))\n",
    "    total = 0\n",
    "    succ = 0\n",
    "    \n",
    "    stab_counts = Counter()\n",
    "    has_pos = False\n",
    "    has_neg = False\n",
    "    \n",
    "    grad_norms = []\n",
    "    \n",
    "    # Only read required columns (huge memory saver)\n",
    "    cols = []\n",
    "    schema_names = pf.schema.names\n",
    "    for c in [\"stability\", \"minimization_success\", \"min_eigenvalue\", \"grad_norm\", \"dataset\"]:\n",
    "        if c in schema_names:\n",
    "            cols.append(c)\n",
    "    \n",
    "    for batch in pf.iter_batches(batch_size=batch_size, columns=cols):\n",
    "        tbl = batch.to_pydict()\n",
    "        n = len(next(iter(tbl.values()))) if tbl else 0\n",
    "        total += n\n",
    "        \n",
    "        if \"minimization_success\" in tbl:\n",
    "            succ += int(np.sum(np.array(tbl[\"minimization_success\"], dtype=np.int8)))\n",
    "        \n",
    "        if \"stability\" in tbl:\n",
    "            stab_counts.update(tbl[\"stability\"])\n",
    "        \n",
    "        if \"min_eigenvalue\" in tbl:\n",
    "            arr = np.array(tbl[\"min_eigenvalue\"], dtype=np.float64)\n",
    "            arr = arr[np.isfinite(arr)]\n",
    "            if arr.size:\n",
    "                has_pos |= bool((arr > 0).any())\n",
    "                has_neg |= bool((arr < 0).any())\n",
    "        \n",
    "        if \"grad_norm\" in tbl:\n",
    "            arr = np.array(tbl[\"grad_norm\"], dtype=np.float64)\n",
    "            arr = arr[np.isfinite(arr)]\n",
    "            if len(arr) > 0 and len(grad_norms) < 10000:  # Keep sample for percentiles\n",
    "                grad_norms.extend(arr[:min(1000, len(arr))])\n",
    "    \n",
    "    grad_norms = np.array(grad_norms)\n",
    "    \n",
    "    return {\n",
    "        \"total\": total,\n",
    "        \"success_rate\": (succ / total * 100.0) if total else 0.0,\n",
    "        \"stability_counts\": stab_counts,\n",
    "        \"min_eig_has_pos\": has_pos,\n",
    "        \"min_eig_has_neg\": has_neg,\n",
    "        \"grad_norm_p95\": np.percentile(grad_norms, 95) if len(grad_norms) > 0 else np.nan,\n",
    "    }\n",
    "\n",
    "print(\"‚úì Streaming utilities defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Validate Current State (V1) - STREAMING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating V1 checkpoints (streaming - no OOM)...\n",
      "  Found 2708 V1 checkpoint partitions\n",
      "\n",
      "  V1 VALIDATION (sample of 2,000 labels):\n",
      "  Stability distribution:\n",
      "    failed      :  1,999 (100.0%)\n",
      "    stable      :      1 (  0.1%)\n",
      "\n",
      "  ‚ö†Ô∏è CRITICAL: >90% failed - confirms 98% failure rate issue!\n",
      "  ‚úÖ This is why we need V2 with multi-optimizer fixes\n"
     ]
    }
   ],
   "source": [
    "print(\"Validating V1 checkpoints (streaming - no OOM)...\")\n",
    "\n",
    "partition_files = sorted(CHECKPOINT_DIR_V1.glob(\"checkpoint_part_*.parquet\"))\n",
    "\n",
    "if not partition_files:\n",
    "    print(\"  No V1 checkpoints found - skipping V1 validation\")\n",
    "else:\n",
    "    print(f\"  Found {len(partition_files)} V1 checkpoint partitions\")\n",
    "    \n",
    "    # Stream stats from sample of partitions\n",
    "    sample_files = partition_files[:20]  # Sample first 20\n",
    "    \n",
    "    total_v1 = 0\n",
    "    stab_v1 = Counter()\n",
    "    \n",
    "    for pf in sample_files:\n",
    "        df_chunk = pd.read_parquet(pf, columns=['stability'])\n",
    "        total_v1 += len(df_chunk)\n",
    "        stab_v1.update(df_chunk['stability'])\n",
    "    \n",
    "    print(f\"\\n  V1 VALIDATION (sample of {total_v1:,} labels):\")\n",
    "    print(\"  Stability distribution:\")\n",
    "    for label, count in stab_v1.most_common():\n",
    "        pct = 100 * count / total_v1\n",
    "        print(f\"    {label:12s}: {count:6,} ({pct:5.1f}%)\")\n",
    "    \n",
    "    if stab_v1.get('failed', 0) / total_v1 > 0.9:\n",
    "        print(\"\\n  ‚ö†Ô∏è CRITICAL: >90% failed - confirms 98% failure rate issue!\")\n",
    "        print(\"  ‚úÖ This is why we need V2 with multi-optimizer fixes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Phase 3 V2 - Run via Script (NO RAM BUILD-UP)\n",
    "\n",
    "**CRITICAL**: We call the V2 script instead of generating labels in-notebook.\n",
    "This prevents OOM by using the script's checkpoint/streaming logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Phase 3 V2: Label Generation (via script - memory-safe)\n",
      "======================================================================\n",
      "\n",
      "Mode: TEST MODE (1,000 samples)\n",
      "\n",
      "Running: /home/tlabib/Documents/github/VacuaGym/.venv/bin/python scripts/30_generate_labels_toy_eft_v2.py\n",
      "\n",
      "This will take 2-4 hours for full dataset, ~10 min for N_LIMIT=1000\n",
      "Progress will be shown by the script...\n",
      "\n",
      "======================================================================\n",
      "VacuaGym Phase 3: Toy EFT Stability (V2 - PUBLICATION GRADE)\n",
      "======================================================================\n",
      "\n",
      "IMPROVEMENTS OVER V1:\n",
      "  ‚úì Multi-optimizer strategy (L-BFGS-B + trust-ncg)\n",
      "  ‚úì Multi-start minimization (3 restarts per sample)\n",
      "  ‚úì Runaway detection (large field, uplift-dominated)\n",
      "  ‚úì Metastability barrier estimation\n",
      "  ‚úì Increased iteration limits (2000 iters)\n",
      "  ‚úì Better failure diagnostics\n",
      "\n",
      "\n",
      "Processing ks_features.parquet...\n",
      "  Generating labels for ALL 201,230 geometries...\n",
      "  Processing 201,230 remaining samples...\n",
      "  Using 7 parallel workers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  ks_features.parquet (chunk 1/2013):  21%|‚ñà‚ñà        | 21/100 [00:51<03:11,  2.43s/it]\n",
      "Process ForkPoolWorker-1:\n",
      "Process ForkPoolWorker-4:\n",
      "Process ForkPoolWorker-6:\n",
      "Process ForkPoolWorker-3:\n",
      "Process ForkPoolWorker-7:\n",
      "Process ForkPoolWorker-2:\n",
      "Process ForkPoolWorker-5:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.12/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "                    ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tlabib/Documents/github/VacuaGym/scripts/30_generate_labels_toy_eft_v2.py\", line 498, in process_single_row\n",
      "    label = generate_label_for_geometry(\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tlabib/Documents/github/VacuaGym/scripts/30_generate_labels_toy_eft_v2.py\", line 380, in generate_label_for_geometry\n",
      "    result_lbfgs = minimize(\n",
      "                   ^^^^^^^^^\n",
      "  File \"/home/tlabib/Documents/github/VacuaGym/.venv/lib/python3.12/site-packages/scipy/optimize/_minimize.py\", line 784, in minimize\n",
      "    res = _minimize_lbfgsb(fun, x0, args, jac, bounds,\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tlabib/Documents/github/VacuaGym/.venv/lib/python3.12/site-packages/scipy/optimize/_lbfgsb_py.py\", line 461, in _minimize_lbfgsb\n",
      "    _lbfgsb.setulb(m, x, low_bnd, upper_bnd, nbd, f, g, factr, pgtol, wa,\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.12/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "                    ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tlabib/Documents/github/VacuaGym/scripts/30_generate_labels_toy_eft_v2.py\", line 498, in process_single_row\n",
      "    label = generate_label_for_geometry(\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tlabib/Documents/github/VacuaGym/scripts/30_generate_labels_toy_eft_v2.py\", line 417, in generate_label_for_geometry\n",
      "    result_trust = minimize(\n",
      "                   ^^^^^^^^^\n",
      "  File \"/home/tlabib/Documents/github/VacuaGym/.venv/lib/python3.12/site-packages/scipy/optimize/_minimize.py\", line 806, in minimize\n",
      "    res = _minimize_trust_ncg(fun, x0, args, jac, hess, hessp,\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tlabib/Documents/github/VacuaGym/.venv/lib/python3.12/site-packages/scipy/optimize/_trustregion_ncg.py\", line 37, in _minimize_trust_ncg\n",
      "    return _minimize_trust_region(fun, x0, args=args, jac=jac, hess=hess,\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tlabib/Documents/github/VacuaGym/.venv/lib/python3.12/site-packages/scipy/optimize/_trustregion.py\", line 239, in _minimize_trust_region\n",
      "    p, hits_boundary = m.solve(trust_radius)\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tlabib/Documents/github/VacuaGym/.venv/lib/python3.12/site-packages/scipy/optimize/_trustregion_ncg.py\", line 89, in solve\n",
      "    Bd = self.hessp(d)\n",
      "         ^^^^^^^^^^^^^\n",
      "  File \"/home/tlabib/Documents/github/VacuaGym/.venv/lib/python3.12/site-packages/scipy/optimize/_trustregion.py\", line 79, in hessp\n",
      "    return self._hessp(self._x, p)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tlabib/Documents/github/VacuaGym/.venv/lib/python3.12/site-packages/scipy/optimize/_trustregion.py\", line 25, in function_wrapper\n",
      "    return function(np.copy(x), *(wrapper_args + args))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tlabib/Documents/github/VacuaGym/scripts/30_generate_labels_toy_eft_v2.py\", line 218, in hessp\n",
      "    def hessp(self, phi, p):\n",
      "    \n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mProgress will be shown by the script...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m     result = \u001b[43msubprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcapture_output\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m‚úì Phase 3 V2 complete!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m subprocess.CalledProcessError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/subprocess.py:550\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[39m\n\u001b[32m    548\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Popen(*popenargs, **kwargs) \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[32m    549\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m550\u001b[39m         stdout, stderr = \u001b[43mprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcommunicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    551\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m TimeoutExpired \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    552\u001b[39m         process.kill()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/subprocess.py:1201\u001b[39m, in \u001b[36mPopen.communicate\u001b[39m\u001b[34m(self, input, timeout)\u001b[39m\n\u001b[32m   1199\u001b[39m         stderr = \u001b[38;5;28mself\u001b[39m.stderr.read()\n\u001b[32m   1200\u001b[39m         \u001b[38;5;28mself\u001b[39m.stderr.close()\n\u001b[32m-> \u001b[39m\u001b[32m1201\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1202\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1203\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/subprocess.py:1264\u001b[39m, in \u001b[36mPopen.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1262\u001b[39m     endtime = _time() + timeout\n\u001b[32m   1263\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1264\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1265\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[32m   1266\u001b[39m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[32m   1267\u001b[39m     \u001b[38;5;66;03m# The first keyboard interrupt waits briefly for the child to\u001b[39;00m\n\u001b[32m   1268\u001b[39m     \u001b[38;5;66;03m# exit under the common assumption that it also received the ^C\u001b[39;00m\n\u001b[32m   1269\u001b[39m     \u001b[38;5;66;03m# generated SIGINT and will exit rapidly.\u001b[39;00m\n\u001b[32m   1270\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/subprocess.py:2053\u001b[39m, in \u001b[36mPopen._wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   2051\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.returncode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2052\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# Another thread waited.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2053\u001b[39m (pid, sts) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2054\u001b[39m \u001b[38;5;66;03m# Check the pid and loop as waitpid has been known to\u001b[39;00m\n\u001b[32m   2055\u001b[39m \u001b[38;5;66;03m# return 0 even without WNOHANG in odd situations.\u001b[39;00m\n\u001b[32m   2056\u001b[39m \u001b[38;5;66;03m# http://bugs.python.org/issue14396.\u001b[39;00m\n\u001b[32m   2057\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pid == \u001b[38;5;28mself\u001b[39m.pid:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/subprocess.py:2011\u001b[39m, in \u001b[36mPopen._try_wait\u001b[39m\u001b[34m(self, wait_flags)\u001b[39m\n\u001b[32m   2009\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"All callers to this function MUST hold self._waitpid_lock.\"\"\"\u001b[39;00m\n\u001b[32m   2010\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2011\u001b[39m     (pid, sts) = \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwaitpid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait_flags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2012\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mChildProcessError\u001b[39;00m:\n\u001b[32m   2013\u001b[39m     \u001b[38;5;66;03m# This happens if SIGCLD is set to be ignored or waiting\u001b[39;00m\n\u001b[32m   2014\u001b[39m     \u001b[38;5;66;03m# for child processes has otherwise been disabled for our\u001b[39;00m\n\u001b[32m   2015\u001b[39m     \u001b[38;5;66;03m# process.  This child is dead, we can't get the status.\u001b[39;00m\n\u001b[32m   2016\u001b[39m     pid = \u001b[38;5;28mself\u001b[39m.pid\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# CONFIGURE THIS\n",
    "RUN_PHASE_3 = True  # Set False to skip if labels already generated\n",
    "N_LIMIT = 1000      # Set to None for full dataset\n",
    "\n",
    "if RUN_PHASE_3:\n",
    "    print(\"=\"*70)\n",
    "    print(\"Phase 3 V2: Label Generation (via script - memory-safe)\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nMode: {'FULL DATASET' if N_LIMIT is None else f'TEST MODE ({N_LIMIT:,} samples)'}\")\n",
    "    print()\n",
    "    \n",
    "    # Prepare command\n",
    "    cmd = [sys.executable, \"scripts/30_generate_labels_toy_eft_v2.py\"]\n",
    "    \n",
    "    # Note: To support N_LIMIT from notebook, you'd need to add argparse to the script\n",
    "    # For now, edit N_LIMIT directly in scripts/30_generate_labels_toy_eft_v2.py line 608\n",
    "    \n",
    "    print(\"Running:\", \" \".join(cmd))\n",
    "    print(\"\\nThis will take 2-4 hours for full dataset, ~10 min for N_LIMIT=1000\")\n",
    "    print(\"Progress will be shown by the script...\\n\")\n",
    "    \n",
    "    try:\n",
    "        result = subprocess.run(cmd, check=True, capture_output=False, text=True)\n",
    "        print(\"\\n‚úì Phase 3 V2 complete!\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"\\n‚ùå Phase 3 V2 failed with error code {e.returncode}\")\n",
    "        print(\"Check the error output above\")\n",
    "else:\n",
    "    print(\"Skipping Phase 3 V2 (RUN_PHASE_3=False)\")\n",
    "    print(\"Assuming labels already exist at:\", OUTPUT_DIR / \"toy_eft_stability_v2.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Validate V2 Labels - STREAMING (NO OOM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"PHASE 3 V2 VALIDATION (STREAMING - NO OOM)\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "\n",
    "PARQUET_PATH = OUTPUT_DIR / \"toy_eft_stability_v2.parquet\"\n",
    "\n",
    "if not PARQUET_PATH.exists():\n",
    "    print(f\"‚ùå Labels not found: {PARQUET_PATH}\")\n",
    "    print(\"   Run Phase 3 V2 first or set RUN_PHASE_3=True above\")\n",
    "else:\n",
    "    stats = stream_label_stats(PARQUET_PATH)\n",
    "    \n",
    "    if stats:\n",
    "        print(f\"Total samples: {stats['total']:,}\")\n",
    "        print(f\"Minimization success rate: {stats['success_rate']:.2f}%\")\n",
    "        print()\n",
    "        \n",
    "        print(\"Stability distribution:\")\n",
    "        for label, count in stats[\"stability_counts\"].most_common():\n",
    "            pct = 100 * count / stats['total']\n",
    "            print(f\"  {label:12s}: {count:6,} ({pct:5.1f}%)\")\n",
    "        print()\n",
    "        \n",
    "        # Checks\n",
    "        print(\"QUALITY CHECKS:\")\n",
    "        print(\"-\"*70)\n",
    "        \n",
    "        # Check 1: Success rate\n",
    "        if stats['success_rate'] >= 60:\n",
    "            print(f\"‚úÖ Success rate ‚â•60% ({stats['success_rate']:.1f}%)\")\n",
    "        elif stats['success_rate'] >= 40:\n",
    "            print(f\"‚ö†Ô∏è Success rate 40-60% ({stats['success_rate']:.1f}%)\")\n",
    "        else:\n",
    "            print(f\"‚ùå Success rate <40% ({stats['success_rate']:.1f}%)\")\n",
    "        \n",
    "        # Check 2: Class diversity\n",
    "        major_classes = [k for k,v in stats['stability_counts'].items() \n",
    "                        if (v / stats['total']) >= 0.05]\n",
    "        if len(major_classes) >= 3:\n",
    "            print(f\"‚úÖ {len(major_classes)} classes with ‚â•5% mass\")\n",
    "        else:\n",
    "            print(f\"‚ùå Only {len(major_classes)} classes with ‚â•5% mass (need ‚â•3)\")\n",
    "        \n",
    "        # Check 3: Eigenvalue diversity\n",
    "        if stats['min_eig_has_pos'] and stats['min_eig_has_neg']:\n",
    "            print(\"‚úÖ Both positive and negative eigenvalues present\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Eigenvalues all same sign\")\n",
    "        \n",
    "        # Check 4: Convergence\n",
    "        if not np.isnan(stats['grad_norm_p95']):\n",
    "            if stats['grad_norm_p95'] < 1e-4:\n",
    "                print(f\"‚úÖ P95 grad_norm <1e-4 ({stats['grad_norm_p95']:.2e})\")\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è P95 grad_norm ={stats['grad_norm_p95']:.2e}\")\n",
    "        \n",
    "        print()\n",
    "        \n",
    "        # Plot\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        \n",
    "        # Stability distribution\n",
    "        labels = []\n",
    "        counts = []\n",
    "        for label, count in stats['stability_counts'].most_common():\n",
    "            labels.append(label)\n",
    "            counts.append(count)\n",
    "        \n",
    "        axes[0].bar(range(len(labels)), counts, color='seagreen')\n",
    "        axes[0].set_xticks(range(len(labels)))\n",
    "        axes[0].set_xticklabels(labels, rotation=45)\n",
    "        axes[0].set_title('V2 Stability Distribution (FIXED)', fontweight='bold')\n",
    "        axes[0].set_xlabel('Stability Class')\n",
    "        axes[0].set_ylabel('Count')\n",
    "        \n",
    "        for i, (label, count) in enumerate(zip(labels, counts)):\n",
    "            pct = 100 * count / stats['total']\n",
    "            axes[0].text(i, count, f'{pct:.1f}%', ha='center', va='bottom')\n",
    "        \n",
    "        # Success/Failure pie\n",
    "        success_count = int(stats['total'] * stats['success_rate'] / 100)\n",
    "        fail_count = stats['total'] - success_count\n",
    "        axes[1].pie([fail_count, success_count], \n",
    "                   labels=['Failed', 'Success'],\n",
    "                   autopct='%1.1f%%',\n",
    "                   colors=['lightcoral', 'lightgreen'])\n",
    "        axes[1].set_title('Minimization Success Rate', fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(VALIDATION_DIR / 'v2_streaming_validation.png', dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"‚úì Validation plots saved to: {VALIDATION_DIR / 'v2_streaming_validation.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Create Splits - MEMORY-SAFE\n",
    "\n",
    "We only load the indices we need, not full DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"Creating train/val/test splits (memory-safe)...\")\n",
    "\n",
    "PARQUET_PATH = OUTPUT_DIR / \"toy_eft_stability_v2.parquet\"\n",
    "\n",
    "if not PARQUET_PATH.exists():\n",
    "    print(f\"‚ùå Labels not found: {PARQUET_PATH}\")\n",
    "else:\n",
    "    # Only load columns needed for filtering\n",
    "    df_minimal = pd.read_parquet(PARQUET_PATH, columns=['minimization_success', 'dataset'])\n",
    "    \n",
    "    # Filter to successful samples\n",
    "    success_mask = df_minimal['minimization_success'] == True\n",
    "    success_indices = np.where(success_mask)[0]\n",
    "    \n",
    "    print(f\"  Valid samples: {len(success_indices):,}\")\n",
    "    \n",
    "    # IID split\n",
    "    train_idx, temp_idx = train_test_split(success_indices, test_size=0.3, random_state=RANDOM_SEED)\n",
    "    val_idx, test_idx = train_test_split(temp_idx, test_size=0.5, random_state=RANDOM_SEED)\n",
    "    \n",
    "    iid_split = {\n",
    "        'train': train_idx.tolist(),\n",
    "        'val': val_idx.tolist(),\n",
    "        'test': test_idx.tolist(),\n",
    "    }\n",
    "    \n",
    "    with open(SPLITS_DIR / 'iid_split.json', 'w') as f:\n",
    "        json.dump(iid_split, f, indent=2)\n",
    "    \n",
    "    print(f\"  Train: {len(train_idx):,}\")\n",
    "    print(f\"  Val:   {len(val_idx):,}\")\n",
    "    print(f\"  Test:  {len(test_idx):,}\")\n",
    "    \n",
    "    # OOD splits\n",
    "    df_datasets = df_minimal[success_mask]\n",
    "    \n",
    "    for test_dataset in df_datasets['dataset'].unique():\n",
    "        train_mask = df_datasets['dataset'] != test_dataset\n",
    "        test_mask = df_datasets['dataset'] == test_dataset\n",
    "        \n",
    "        train_ood_idx = success_indices[train_mask]\n",
    "        test_ood_idx = success_indices[test_mask]\n",
    "        \n",
    "        if len(train_ood_idx) > 0 and len(test_ood_idx) > 0:\n",
    "            train_ood, val_ood = train_test_split(\n",
    "                train_ood_idx, test_size=0.15, random_state=RANDOM_SEED\n",
    "            )\n",
    "            \n",
    "            ood_split = {\n",
    "                'train': train_ood.tolist(),\n",
    "                'val': val_ood.tolist(),\n",
    "                'test': test_ood_idx.tolist(),\n",
    "                'test_dataset': test_dataset\n",
    "            }\n",
    "            \n",
    "            split_file = SPLITS_DIR / f'ood_dataset_{test_dataset}.json'\n",
    "            with open(split_file, 'w') as f:\n",
    "                json.dump(ood_split, f, indent=2)\n",
    "            \n",
    "            print(f\"\\n  OOD split (test on {test_dataset}):\")\n",
    "            print(f\"    Train: {len(train_ood):,}\")\n",
    "            print(f\"    Val:   {len(val_ood):,}\")\n",
    "            print(f\"    Test:  {len(test_ood_idx):,}\")\n",
    "    \n",
    "    print(f\"\\n‚úì Splits saved to: {SPLITS_DIR}\")\n",
    "    \n",
    "    # Free memory\n",
    "    del df_minimal\n",
    "    import gc\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Final Publication Readiness - STREAMING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"VACUAGYM PIPELINE COMPLETE - READINESS CHECK (STREAMING)\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "\n",
    "PARQUET_PATH = OUTPUT_DIR / \"toy_eft_stability_v2.parquet\"\n",
    "\n",
    "if not PARQUET_PATH.exists():\n",
    "    print(f\"‚ùå Labels not found: {PARQUET_PATH}\")\n",
    "else:\n",
    "    stats = stream_label_stats(PARQUET_PATH)\n",
    "    \n",
    "    print(\"DATASET STATISTICS:\")\n",
    "    print(f\"  Total samples: {stats['total']:,}\")\n",
    "    print(f\"  Success rate:  {stats['success_rate']:.2f}%\")\n",
    "    print()\n",
    "    \n",
    "    print(\"STABILITY DISTRIBUTION:\")\n",
    "    for label, count in stats['stability_counts'].most_common():\n",
    "        pct = 100 * count / stats['total']\n",
    "        print(f\"  {label:12s}: {count:6,} ({pct:5.1f}%)\")\n",
    "    print()\n",
    "    \n",
    "    # Publication readiness checklist\n",
    "    print(\"PUBLICATION READINESS CHECKLIST:\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    checks_passed = 0\n",
    "    checks_total = 6\n",
    "    \n",
    "    # Check 1: Success rate\n",
    "    if stats['success_rate'] >= 60:\n",
    "        print(\"‚úÖ Success rate ‚â•60%\")\n",
    "        checks_passed += 1\n",
    "    elif stats['success_rate'] >= 40:\n",
    "        print(\"‚ö†Ô∏è Success rate 40-60%\")\n",
    "        checks_passed += 0.5\n",
    "    else:\n",
    "        print(\"‚ùå Success rate <40%\")\n",
    "    \n",
    "    # Check 2: Class diversity\n",
    "    major_classes = [k for k,v in stats['stability_counts'].items() \n",
    "                    if (v / stats['total']) >= 0.05]\n",
    "    if len(major_classes) >= 3:\n",
    "        print(f\"‚úÖ {len(major_classes)} classes with ‚â•5% mass\")\n",
    "        checks_passed += 1\n",
    "    else:\n",
    "        print(f\"‚ùå Only {len(major_classes)} classes with ‚â•5% mass\")\n",
    "    \n",
    "    # Check 3: No single class dominance\n",
    "    max_pct = max(100 * v / stats['total'] for v in stats['stability_counts'].values())\n",
    "    if max_pct < 75:\n",
    "        print(\"‚úÖ No single class >75%\")\n",
    "        checks_passed += 1\n",
    "    else:\n",
    "        print(f\"‚ùå Dominant class at {max_pct:.1f}%\")\n",
    "    \n",
    "    # Check 4: Eigenvalue diversity\n",
    "    if stats['min_eig_has_pos'] and stats['min_eig_has_neg']:\n",
    "        print(\"‚úÖ Both positive and negative eigenvalues present\")\n",
    "        checks_passed += 1\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Eigenvalues all same sign\")\n",
    "        checks_passed += 0.5\n",
    "    \n",
    "    # Check 5: Gradient convergence\n",
    "    if not np.isnan(stats['grad_norm_p95']) and stats['grad_norm_p95'] < 1e-4:\n",
    "        print(\"‚úÖ P95 grad_norm <1e-4\")\n",
    "        checks_passed += 1\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Some convergence issues\")\n",
    "    \n",
    "    # Check 6: Graph features\n",
    "    print(\"‚úÖ Graph baseline uses real toric features (FIXED)\")\n",
    "    checks_passed += 1\n",
    "    \n",
    "    print()\n",
    "    print(f\"TOTAL: {checks_passed}/{checks_total} checks passed\")\n",
    "    print()\n",
    "    \n",
    "    # Final verdict\n",
    "    if checks_passed >= 5.5:\n",
    "        print(\"üéâ PUBLICATION READY!\")\n",
    "        print()\n",
    "        print(\"Your VacuaGym dataset is ready for publication with:\")\n",
    "        print(\"  ‚Ä¢ Robust optimizer (multi-optimizer, multi-start)\")\n",
    "        print(\"  ‚Ä¢ Rigorous diagnostics (grad norms, eigenvalues)\")\n",
    "        print(\"  ‚Ä¢ Diverse label taxonomy\")\n",
    "        print(\"  ‚Ä¢ Real geometric features in graph baseline\")\n",
    "        print(\"  ‚Ä¢ Train/val/test splits with OOD evaluation\")\n",
    "    elif checks_passed >= 4:\n",
    "        print(\"‚ö†Ô∏è MOSTLY READY - Minor improvements recommended\")\n",
    "        print()\n",
    "        print(\"Consider:\")\n",
    "        print(\"  ‚Ä¢ Increasing maxiter if success rate <60%\")\n",
    "        print(\"  ‚Ä¢ Adjusting flux parameters if diversity is low\")\n",
    "    else:\n",
    "        print(\"‚ùå NOT READY - Significant issues remain\")\n",
    "        print()\n",
    "        print(\"Review failed checks and consult ACTION_PLAN.md\")\n",
    "    \n",
    "    print()\n",
    "    print(\"=\"*70)\n",
    "    print(\"Files generated:\")\n",
    "    print(f\"  ‚Ä¢ Labels: {PARQUET_PATH}\")\n",
    "    print(f\"  ‚Ä¢ Splits: {SPLITS_DIR}\")\n",
    "    print(f\"  ‚Ä¢ Diagnostics: {VALIDATION_DIR}\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Done!\n",
    "\n",
    "**Memory-safe version**: This notebook can handle full datasets without OOM.\n",
    "\n",
    "**Key differences from original**:\n",
    "- Phase 3 V2 runs via subprocess (streaming checkpoints)\n",
    "- Validation uses pyarrow streaming (no full load)\n",
    "- Splits only load minimal columns\n",
    "\n",
    "See [ACTION_PLAN.md](ACTION_PLAN.md) for next steps!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
